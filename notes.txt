
The original attention is all you need paper published Encoder-Decoder architecture which can be used to translate english sentences to german sentences.

The encoder model is widely used to take in an input sentence and output rich, context-sensitive representation of the input. It is used in embedding models such as BERT

The decoder model takes in the input prompt and generates a high quality sentences, which is what the popular LLM's do.

Language models

	1. Non-transformer models

		- Bag of words (~2000)

			Split documents by a white-space character. Build vocabulary by performing a unique() operation on all words. Now for each document go through the entire vocabulary vector and count the word occurrences (including words which doesn't appear in this particular document). The resulting numerical counts of the entire vocabulary vector is called bag-of-words representation! Note: Order of vocabulary vector is important

		- Word2Vec (~2013)

			Bag of words lacks semantic reference of the underlying documents. Word2Vec uses neural networks to capture the meaning of documents.

			Word2Vec generates an high-dimensional vector embedding of the sentence it operates on!

		- Attention (2017)

			Word2Vec captures the semantic meaning, but not within the context. For examples, the word "bank" means different things in different contexts.

			We use RNN's do capture the context as well. They operate on sequences of words. They are called "auto-regressive" that means to generate a single output token, we rely on the entire input tokens.

			Note: The input to these RNN's are the output vector embeddings generated by Word2Vec

			"Attention" was later proposed to further optimize the RNN architecture. The core idea is "Attention" allows a model to focus on parts of the input that are relevant to one another.

	2. Encoder-only models

		- BERT (2018)

			Encoder block in BERT have self-attention layer followed by feed-forward network. You can think of them as a more advanced form of Word2Vec. They take in input sentence and output contextualized word embeddings

		- DistilBERT and RoBERTA (~2020)

	3. Decoder-only models

		- All GPT-based models (GPT in 2018, GPT-2 in 2019 and GPT-3 in 2021)

			We take in input sentences with randomly initialized word embeddings. The decoder block uses masked-self attention layer followed by a feed-forward neural network.

			Masked-self-attention layer is basically a lower-triangular matrix, with the upper-triangular matrix set to all zeroes. This is because we only need to pay attention to words which came earlier, unlike encoder-only models which need full-attention to generate contextualized representations.

			Decoder-only models are also called generative models, whereas encoder-only models are called representational models.

			Note: The last layer of the decoder-only models is called "LM Head", i.e., language modelling head. It's an output vector with the same size as the total token-vocabulary size used at the input stage (i.e., in the tokenizer). Each element in the vector represents the probability (between 0 to 1) which the decoder-only network has determined. We can either choose the highest probability value, or opt for a "temperature" based mechanism which will randomly choose from the top 10 highly probably token to improve creativity of our model.

			Decoder-only models are also highly-parallelizable which is the reason why they are more popular and practical than their encoder-only or encoder-decoder counterparts.

			If the context size is 16000, then the decoder only model can process 16000 tokens in parallel and generate the next 16001-th token.

			Additionally, when we are feeding back the 16001-th token to generate the next token, we can make use of cached calculations in the network to further speed-up generation.

	4. Encoder-Decoder models

		- T5, Switch, and Flan-T5

Mixture of Experts (MoE)

	These are a variant of transformer models (i.e. improvement/optimization). We introduce a router which will route our input to one of the N experts (which are sub-models) at EVERY LAYER of the network. This is in contrast to ensemble models where all the models are standalone ML models and we take the best possible output!

	One example is to think of the experts in MoE as focusing on
	- Punctuation
	- Verbs
	- Conjunctions
	- Visual descriptions

	The router is a classifier model, which must be trained separately.

